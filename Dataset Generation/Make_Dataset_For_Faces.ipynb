{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare reading datasets, metadata texts\n",
    "prefix_txt = \"landmarks.\"\n",
    "prefix_img = \"coarse_tilt_aligned_face.\"\n",
    "cwd = os.getcwd()+\"/\"\n",
    "textfiles = ['fold_0_data.txt','fold_1_data.txt','fold_2_data.txt','fold_3_data.txt','fold_4_data.txt']\n",
    "\n",
    "# Since there are labels that do not match the classes stated, need to fix them\n",
    "classes = [\"(0, 2)\", \"(4, 6)\", \"(8, 12)\", \"(15, 20)\", \"(25, 32)\", \"(38, 43)\", \"(48, 53)\", \"(60, 100)\"]\n",
    "fix_dict = {'35': classes[5], '3': classes[0], '55': classes[7], '58': classes[7], \n",
    "'22': classes[3], '13': classes[2], '45': classes[5], '36': classes[5], \n",
    "'23': classes[4], '57': classes[7], '56': classes[6], '2': classes[0], \n",
    "'29': classes[4], '34': classes[4], '42': classes[5], '46': classes[6], \n",
    "'32': classes[4], '(38, 48)': classes[5], '(38, 42)': classes[5], '(8, 23)': classes[2],\n",
    " '(27, 32)': classes[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "none_count = 0\n",
    "def return_five_cross_validation(textfile):\n",
    "    global none_count\n",
    "    # one big folder list\n",
    "    folder = []\n",
    "    # start processing txt\n",
    "    with open(textfile) as text:\n",
    "        lines = text.readlines()\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        # real image path from folder\n",
    "        img_path = line[0]+\"/\"+prefix_img+line[2]+\".\"+line[1]\n",
    "        landmark_txt_path = line[0]+\"/\"+prefix_txt+line[2]+\".\"+line[1][:-3]+\"txt\"\n",
    "        if line[3] == \"None\":\n",
    "            none_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            folder.append([img_path]+[landmark_txt_path]+line[3:5]+[line[-2]])\n",
    "            if folder[-1][2] in fix_dict:\n",
    "                folder[-1][2] = fix_dict[folder[-1][2]]\n",
    "    return folder\n",
    "\n",
    "\n",
    "all_folders = []\n",
    "for textfile in textfiles:\n",
    "    folder = return_five_cross_validation(textfile)\n",
    "    all_folders.append(folder)\n",
    "print(\"A sample:\", all_folders[0][0])\n",
    "print(\"No. of Pics without Age Group Label:\", none_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for processing img arrays, landmarks and one-hot generation\n",
    "def read_landmark_file(filename):\n",
    "    f = open(filename, 'r')\n",
    "    points = []\n",
    "    for line in f.readlines()[2:]:\n",
    "        line = line.strip().split(\",\")[-2:]\n",
    "        points.append(np.array(line, dtype=float).astype(int))\n",
    "    f.close()\n",
    "    return points\n",
    "\n",
    "def imread(path, width, height):\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "def build_one_hot(age):\n",
    "    label = np.zeros(len(classes), dtype=int)\n",
    "    label[classes.index(age)] = 1\n",
    "    return label\n",
    "\n",
    "def resize(img, points=None, width=224, height=224):\n",
    "    # translate landmark\n",
    "    resize_ratio_x = width/img.shape[1]\n",
    "    resize_ratio_y = height/img.shape[0]\n",
    "    for i in range(len(points)):\n",
    "        points[i][0] = int(resize_ratio_x*points[i][0])\n",
    "        points[i][1] = int(resize_ratio_y*points[i][1])\n",
    "    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    return img, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image size\n",
    "width, height = 224, 224\n",
    "\n",
    "# loop for reading imgs from five folders\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_landmark_points = []\n",
    "print(\"Start reading images data...\")\n",
    "for folder in all_folders:\n",
    "    data = []\n",
    "    labels = []\n",
    "    landmark_points = []\n",
    "    for i in tqdm.tqdm(range(len(folder))):    # here using tqdm to monitor progress\n",
    "        img = imread(folder[i][0], width, height)\n",
    "        points = read_landmark_file(folder[i][1])\n",
    "        one_hot = build_one_hot(folder[i][2])\n",
    "        img, points = resize(img, points, width, height)\n",
    "        data.append(img)\n",
    "        labels.append(one_hot)\n",
    "        landmark_points.append(points)\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "    all_landmark_points.append(landmark_points)\n",
    "    print(\"One folder done...\")\n",
    "print(\"All done!\")\n",
    "\n",
    "img = all_data[0][15].copy()\n",
    "for point in all_landmark_points[0][15]:\n",
    "    cv2.rectangle(img, (point[0], point[1]),(point[0]+2, point[1]+2),(255,0,0),1)\n",
    "plt.imshow(img)\n",
    "\n",
    "#plt.subplot(131),plt.imshow(all_data[0][0]),plt.title(all_labels[0][0])\n",
    "#plt.subplot(132),plt.imshow(all_data[1][0]),plt.title(all_labels[1][0])\n",
    "#plt.subplot(133),plt.imshow(all_data[2][0]),plt.title(all_labels[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = all_data[0][700].copy()\n",
    "for point in all_landmark_points[0][700]:\n",
    "    cv2.rectangle(img, (point[0], point[1]),(point[0]+2, point[1]+2),(255,0,0),1)\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(131),plt.imshow(all_data[0][15]),plt.title(all_labels[0][15])\n",
    "plt.subplot(132),plt.imshow(all_data[1][0]),plt.title(all_labels[1][0])\n",
    "plt.subplot(133),plt.imshow(all_data[2][0]),plt.title(all_labels[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculation of channel-wise BGR means for five folders\n",
    "b_folders = []\n",
    "g_folders = []\n",
    "r_folders = []\n",
    "n_images_folders = []\n",
    "\n",
    "# First we summarize rgb values\n",
    "for i in tqdm.tqdm(range(0, 5)):\n",
    "    b = np.zeros((height, width))\n",
    "    g = np.zeros((height, width))\n",
    "    r = np.zeros((height, width))\n",
    "    for img in all_data[i]:\n",
    "        b += img[:,:,0]\n",
    "        g += img[:,:,1]\n",
    "        r += img[:,:,2]\n",
    "    b_folders.append(b)\n",
    "    g_folders.append(g)\n",
    "    r_folders.append(r)\n",
    "    n_images_folders.append(len(all_data[i]))\n",
    "\n",
    "# Then we generate BGR mean for each cross validation situation\n",
    "# eg. When we validate folder 1, RGB mean will be generated from folder 2~5\n",
    "bgr_means = []\n",
    "for i in range(0, 5):\n",
    "    folders = [0,1,2,3,4]\n",
    "    folders.remove(i)\n",
    "    b = np.zeros((height, width))\n",
    "    g = np.zeros((height, width))\n",
    "    r = np.zeros((height, width))\n",
    "    n_image = 0\n",
    "    for folder_index in folders:\n",
    "        b += b_folders[folder_index]\n",
    "        g += g_folders[folder_index]\n",
    "        r += r_folders[folder_index]\n",
    "        n_image += n_images_folders[folder_index]\n",
    "    bgr_means.append(np.array([np.mean(b/n_image), np.mean(g/n_image), np.mean(r/n_image)]))\n",
    "    \n",
    "print(\"BGR Means Array:\", bgr_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate mean image for each cross validation situation\n",
    "# eg. When we validate folder 1, RGB mean will be generated from folder 2~5\n",
    "mean_imgs = []\n",
    "for i in tqdm.tqdm(range(0, 5)):\n",
    "    folders = [0,1,2,3,4]\n",
    "    folders.remove(i)\n",
    "    mean_image = np.zeros(all_data[0][0].shape)\n",
    "    n_image = 0\n",
    "    for folder_index in folders:\n",
    "        for img in all_data[folder_index]:\n",
    "            mean_image += img\n",
    "        n_image += n_images_folders[folder_index]\n",
    "    mean_imgs.append(np.array(mean_image/n_image, dtype=np.uint8))\n",
    "\n",
    "print(mean_imgs[0].shape, mean_imgs[1].dtype)\n",
    "plt.subplot(121),plt.imshow(mean_imgs[0])\n",
    "plt.subplot(122),plt.imshow(mean_imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate h5py dataset\n",
    "with h5py.File('faces_dataset.h5', 'w') as f:\n",
    "    for i in range(0, 5):\n",
    "        dset_face = f.create_dataset(\"data_\"+str(i+1), data = np.array(all_data[i]))\n",
    "        dset_headers = f.create_dataset('labels_'+str(i+1), data = np.array(all_labels[i]))\n",
    "    dst_bgr_means = f.create_dataset('bgr_means', data = np.array(bgr_means))\n",
    "    #dst_mean_imgs = f.create_dataset('mean_imgs', data = np.array(mean_imgs))\n",
    "print(\"Generation Success!\")\n",
    "\n",
    "with h5py.File('faces_landmark.h5', 'w') as f:\n",
    "    for i in range(0, 5):\n",
    "        dset_landmark = f.create_dataset(\"landmark_\"+str(i+1), data = np.array(all_landmark_points[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
